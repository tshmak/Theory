%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {
	
	% The Beamer class comes with a number of default slide themes
	% which change the colors and layouts of slides. Below this is a list
	% of all the themes, uncomment each in turn to see what they look like.
	
	%\usetheme{default}
	%\usetheme{AnnArbor}
	%\usetheme{Antibes}
	%\usetheme{Bergen}
	%\usetheme{Berkeley}
	%\usetheme{Berlin}
	%\usetheme{Boadilla}
	%\usetheme{CambridgeUS}
	%\usetheme{Copenhagen}
	%\usetheme{Darmstadt}
	%\usetheme{Dresden}
	%\usetheme{Frankfurt}
	%\usetheme{Goettingen}
	%\usetheme{Hannover}
	%\usetheme{Ilmenau}
	%\usetheme{JuanLesPins}
	%\usetheme{Luebeck}
	\usetheme{Madrid}
	%\usetheme{Malmoe}
	%\usetheme{Marburg}
	%\usetheme{Montpellier}
	%\usetheme{PaloAlto}
	%\usetheme{Pittsburgh}
	%\usetheme{Rochester}
	%\usetheme{Singapore}
	%\usetheme{Szeged}
	%\usetheme{Warsaw}
	
	% As well as themes, the Beamer class has a number of color themes
	% for any slide theme. Uncomment each of these in turn to see how it
	% changes the colors of your current slide theme.
	
	%\usecolortheme{albatross}
	%\usecolortheme{beaver}
	%\usecolortheme{beetle}
	%\usecolortheme{crane}
	%\usecolortheme{dolphin}
	%\usecolortheme{dove}
	%\usecolortheme{fly}
	%\usecolortheme{lily}
	%\usecolortheme{orchid}
	%\usecolortheme{rose}
	%\usecolortheme{seagull}
	%\usecolortheme{seahorse}
	%\usecolortheme{whale}
	%\usecolortheme{wolverine}
	
	%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
	\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line
	
	\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%\usepackage {tikz}
%\usepackage {xcolor}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[]{The Information Bottleneck Principle and Deep Learning} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Timothy Mak} % Your name
\institute[Fano Labs] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
	Fano Labs, Hong Kong \\ % Your institution for the title page
	\medskip
}
\date{6/8/2019} % Date, can be changed to a custom date
\DeclareMathOperator{\E}{\mathbb{E}}


\begin{document}
	
	\begin{frame}
	\titlepage % Print the title page as the first slide
	\end{frame}

\begin{frame}
Ideas taken from 3 main papers from Naftali Tishby (Hebrew University) and colleagues 
\begin{itemize}
	\item Tishby and Zaslavsky (2015) \url{https://arxiv.org/abs/1503.02406}
	\item{} [unpublished] Shwartz-Ziv and Tishby (2017) \url{https://arxiv.org/abs/1703.00810}
	\item{} [unpublished] Shwartz-Ziv, Painsky, and Tishby (2018) \url{https://openreview.net/forum?id=SkeL6sCqK7}
\end{itemize}

\end{frame}


%\begin{frame}
%\frametitle{Overview} % Table of contents slide, comment this block out to remove it
%\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
%\end{frame}
%
%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------

\section{A non-technical overview}
\begin{frame}{A non-technical overview}
\begin{itemize}
	\item The quantity $I(X;Y)$ is called the \emph{mutual information} (MI) between random variables $X$ and $Y$, and can be thought of as a general measure of \emph{dependence} between the two variables. The higher it is, the more dependent (correlated) the two variables are. 
	\item In general prediction problems, we are interested in finding a representation $\hat{X}$ of our data $X$ 
	\item The \emph{Information Bottleneck} (IB) principle is that we find $\hat{X} := h(X)$ so as to maximize $I(\hat{X};Y)$ while minimize $I(\hat{X};X)$, where $Y$ is the label of our data. In other words we find the best function $h$ over all possible $h$ as constrained by our neural network setup. 
	\item It is intuitive that maximizing $I(\hat{X};Y)$ should lead to improved prediction performance. However, Tishby and colleagues showed that minimizing $I(\hat{X};X)$ is also associated with better prediction performance. 
\end{itemize}
\end{frame}

\begin{frame}{A non-technical overview}
\begin{itemize}
	\item What Tishby and colleagues showed was that what deep learning (DL) is doing via stochastic gradient descent (SGD) is that it is minimizing over the IB loss function $I(\hat{X};X) - \beta I(\hat{X};Y)$, somewhat unwittingly, which explains its good performance.
	\item SGD is key to the success. We can't replace with another non-stochastic optimizer. 
	\item If we trace $I(\hat{X};X)$ and $I(\hat{X};Y)$ as we train our Neural Network (NN), we find that there are two phases: The first phase is characterized by an increase in $I(\hat{X};Y)$ whereas the second phase is characterized by a decrease in $I(\hat{X};X)$. The second phase is much longer than the first. 
	\item SGD, by its ``stochastic'' nature, decouples $\hat{X}$ from $X$, leading to a decrease in $I(\hat{X};X)$. 
	\item In practice, if we are happy with our strategy in estimating $I(\hat{X};X)$ and $I(\hat{X};Y)$, we can see whether our NN has ``converged'' to its optimal state. 
\end{itemize}
\end{frame}

\begin{frame}{An illustration}
	\url{https://www.youtube.com/watch?v=P1A1yNsxMjc&feature=youtu.be}. 
\end{frame}

\begin{frame}{Recent results from another group (Cheng et al, 2019)}
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{entropy-21-00456-g005}
	%\caption{}
	\label{fig:entropy-21-00456-g005}
\end{figure}
\end{frame}


\section{Deep learning and Artificial Neural Networks}
\begin{frame}{Deep learning and Artificial Neural Networks}
\begin{itemize}
	\item Suppose the relationship between our data $X$ and labels $Y$ is given by the equation $Y = f(X) + \varepsilon$. 
	\item We do not know $f()$ so we try to estimate it with some function $\hat{f}(X; \boldsymbol{\theta})$
	\item The goal of \emph{training} is to try to find good values of $\boldsymbol{\theta}$. 
\end{itemize}
\end{frame}

\begin{frame}{Deep learning and Artificial Neural Networks}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{1_ZB6H4HuF58VcMOWbdpcRxQ}
%	\caption{}
	\label{fig:1zb6h4huf58vcmowbdpcrxq}
\end{figure}
\end{frame}

\begin{frame}{Deep learning and Artificial Neural Networks}
A basic neural network: 
\begin{itemize}
	\item For each layer $k$ and neuron $i$, $T_{k+1,i} = \sigma(\boldsymbol{w}_{ki}^T \boldsymbol{T}_{k} + b_k)
		 = \sigma\left (\sum\limits_{i} T_{k,i} w_{ki} + b_k \right)$
	\item $\boldsymbol{\theta} = \{ w_{ki}, b_k \}$
\end{itemize}
\end{frame}

\begin{frame}{Traditional optimization vs. Stochastic Gradient Descent}
\begin{itemize}
	\item Traditionally, we estimate $\boldsymbol{\theta}$ by minimizing the \emph{empirical loss}. For example, in linear regression, we find $b_0, b_1$ which minimizes $\sum_i (y_i - b_0 - b_1x_i)^2$. 
	\item For large problems this is not feasible, because 
	\begin{enumerate}
		\item $\hat{f}()$ is too complicated
		\item $\hat{\boldsymbol{\theta}}$ is not unique
		\item Too expensive computationally
	\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Traditional optimization vs. Stochastic Gradient Descent}
\begin{itemize}
	\item Instead, Stochastic Gradient Descent (SGD) is often used. 
	\item In SGD, we 
	\begin{enumerate}
		\item Start with some initial values for $\boldsymbol{\theta}$
		\item Take a random sample (mini-batch) of the dataset
		\item Calculate the direction where $\boldsymbol{\theta}$ should change (the gradient) based on the mini-batch sample
		\item Updates $\boldsymbol{\theta}$ by moving it in that direction by an arbitrary small quantity
		\item Keep repeating this until we want to stop
	\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Traditional optimization vs. Stochastic Gradient Descent}
Some features of SGD:
\begin{itemize}
	\item No unique solution
	\item If the mini-batches are non-overlapping, then 1 epoch is when we've gone through the dataset once. 
	\item We obtain the final results after a number of epochs
	\item No guarantee it's the best solution at all! 
\end{itemize}
\end{frame}

\begin{frame}{So why is it a good idea to do SGD?}
	\begin{itemize}
		\item Tishby and colleagues argued that in addition to the advantage of SGD given above, it actually has another advantage --- it produces solutions that are especially immune to ``overfitting''. 
		\item In fact, they said that by running SGD, what we are doing is in fact minimizing the \emph{Information Bottleneck} functional 
		\[
			\mathcal{L}(\hat{X}) = I(\hat{X};X) - \beta I(\hat{X};Y)
		\]
		i.e. we minimize $I(\hat{X};X)$ while maximizing $I(\hat{X};Y)$, where $\hat{X}:=\hat{f}(X; \boldsymbol{\theta})$. 
		\end{itemize}
\end{frame}

\begin{frame}{Why does it make sense to minimize the IB functional?}
\begin{itemize}
	\item Recall that the MI represents dependence or correlation between variables. So naturally, it makes sense to maximize $I(\hat{X};Y)$. 
	\item However, the problem is we don't actually know the distribution $p(X,Y)$, and hence we can't directly maximize $I(\hat{X};Y)$. 
	\item We can, however, maximize $\hat{I}(\hat{X};Y)$, based on the \emph{empirical} distribution $\hat{p}(X,Y)$, i.e. the particular dataset we have. 
	\item Nonetheless, a high $\hat{I}(\hat{X};Y)$ does not imply a high $I(\hat{X};Y)$. In fact, $\hat{I}(\hat{X};Y)$ can be a lot higher than $I(\hat{X};Y)$, and this is called \emph{overfitting}. 
\end{itemize}
\end{frame}

\begin{frame}{What happens in SGD...}
\begin{itemize}
	\item What Tishby and colleagues showed is that the difference between $\hat{I}(\hat{X};Y)$  and $I(\hat{X};Y)$ is in fact related to $I(X;\hat{X})$. 
	\item More specifically, with probability $1 - \delta$
	\[
	|L^{\text{dataset}}(\hat{X}, X) - L^{\text{population}}(\hat{X}, X)|^2 \leq \frac{2^{I(\hat{X};X)} + \log 2/\delta}{2m}
	\]
	where $L(\hat{X}, X)$ is an arbitrary loss function. (An example of a loss function is the squared error.)
	\item $|L^{\text{dataset}}(\hat{X}, X) - L^{\text{population}}(\hat{X}, X)|$ is known as the \emph{generalization gap}. Hence, the generalization gap is bounded by a function of $I(\hat{X};X)$. 
	\item In particular, if $I(\hat{X};X) \ll \log_2 2m$, then we have a strong bound, i.e. limited overfitting. 
\end{itemize}
\end{frame}

\begin{frame}{An intuitive explanation}
\begin{itemize}
	\item In computational learning theory, the more ``flexible'' a function is, the more prone it is to overfitting. Since $\boldsymbol{\theta}$ is very large for deep learning problems, $\hat{f}(\boldsymbol{\theta})$ is very flexible and hence very prone to overfitting (if we don't use SGD.)
	\item Recall in SGD, the solution is not unique. In fact, it depends on the sequence of mini-batches used. Hence, if $\hat{X}$ represents a particular layer in the Neural Network, $\hat{X}$ can be thought of as random (since the mini-batches are random). 
\end{itemize}
\end{frame}

\begin{frame}{An intuitive explanation}
\begin{itemize}
	\item On the other hand, $\hat{X}$ is clearly a function of $X$. Thus, it is partly determined by $X$ and partly random. Loosely speaking we can write
	\[
	\hat{X} = \hat{f}(X) = h(X) + R
	\]
	where $R$ denotes a random element. If the contribution of $R$ is large relative to $h(X)$, $I(\hat{X};X))$ is small. When it is small relative to $h(X)$, $I(\hat{X};X)$ is large.
	\item How ``flexible'' $\hat{f}$ is is related to the relative importance of $h(X)$. If the contribution of $h(X)$ is small, it doesn't matter what $X$ is and the distribution of $\hat{X}$ will be similar. (Imagine the extreme scenario where $\hat{X}$ and $X$ are completely independent.)
	\item On the other hand, if the contribution of $R$ is small, then $\hat{f}$ is highly flexible and thus prone to overfit. 
\end{itemize}
\end{frame}

\begin{frame}{An intuitive explanation}
\begin{itemize}
	\item What happens in SGD is that it increases the contribution of $R$ relative to $h(X)$ and hence decreases $I(\hat{X};X)$. 
	\item If you are familiar with the geometry of optimization, you can think of SGD as essentially a \emph{diffusion process} along the \emph{valley} of equally good fit. 
\end{itemize}
\end{frame}

\begin{frame}{Some evidence this is happening}
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{two_phase}
	%\caption{}
	\label{fig:twophase}
\end{figure}
\end{frame}

\begin{frame}{Some further results}
\begin{itemize}
	\item The exact mathematical details are too difficult so I'll just state: 
	\begin{enumerate}
		\item The different neurons in $T_k$ converge to independence with increasing number of neurons in each layer. 
		\item Different hidden layers converge to different  $I(X;T_k)$.
		\item The main benefit of having more hidden layers is computational. Convergence of $I(X;T_k)$ speeds up markedly with increasing number of layers.
	\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Some further results}
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{6-Figure5-1}
	%\caption{}
	\label{fig:6-figure5-1}
\end{figure}
\end{frame}

\begin{frame}{Some further results}
Sample size matters... 
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{opt_phase2}
	%\caption{}
	\label{opt_phase2}
\end{figure}
\end{frame}


\begin{frame}{More details}
\begin{itemize}
	\item \url{https://portal.fanoai.cn/fkb/display/RES/Deep+learning+and+the+Information+Bottleneck+principle} 
	\item \url{https://portal.fanoai.cn/fkb/display/RES/2019-07-16+Progress+update}
\end{itemize}
\end{frame}

\end{document}

